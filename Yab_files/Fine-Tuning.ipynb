{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54828619-67bb-4884-a4a8-132815410935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150c0069-c4e4-46f5-84ae-f8a3e4626db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA A100 80GB PCIe', major=8, minor=0, total_memory=81037MB, multi_processor_count=108)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299ddd35-82bb-4928-b804-3afdd706c258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfaa270-3b6f-4d19-a767-7835ef21a690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb30e4ee-aaed-4c5e-aaa8-f8cfac6eec9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f243bd7-012d-45b8-bfb0-2380a6916ef7",
   "metadata": {},
   "source": [
    "### Pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ef2f54-e256-433c-a722-db6075325342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers.git@main\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision main) to /tmp/pip-req-build-qjok0_45\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-qjok0_45\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit e34da3ee3c9d2d628fdbeb60cee45c4f8f32945a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bitsandbytes in /student/opt534/.local/lib/python3.10/site-packages (0.43.0)\n",
      "Requirement already satisfied: accelerate==0.27.2 in /student/opt534/.local/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /student/opt534/.local/lib/python3.10/site-packages (from accelerate==0.27.2) (0.4.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from accelerate==0.27.2) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from accelerate==0.27.2) (22.0)\n",
      "Requirement already satisfied: huggingface-hub in /student/opt534/.local/lib/python3.10/site-packages (from accelerate==0.27.2) (0.21.4)\n",
      "Requirement already satisfied: psutil in /usr/local/anaconda3/lib/python3.10/site-packages (from accelerate==0.27.2) (5.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from accelerate==0.27.2) (1.23.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/anaconda3/lib/python3.10/site-packages (from accelerate==0.27.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda3/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /student/opt534/.local/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /student/opt534/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (11.7.101)\n",
      "Requirement already satisfied: networkx in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (2.8.4)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (2.14.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (11.10.3.66)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (8.5.0.96)\n",
      "Requirement already satisfied: sympy in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (1.11.1)\n",
      "Requirement already satisfied: wheel in /usr/local/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.27.2) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.27.2) (65.6.3)\n",
      "Requirement already satisfied: lit in /usr/local/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.27.2) (16.0.3)\n",
      "Requirement already satisfied: cmake in /usr/local/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.27.2) (3.26.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch>=1.10.0->accelerate==0.27.2) (1.2.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/peft.git@e536616888d51b453ed354a6f1e243fecb02ea08\n",
      "  Cloning https://github.com/huggingface/peft.git (to revision e536616888d51b453ed354a6f1e243fecb02ea08) to /tmp/pip-req-build-a42__ghe\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-a42__ghe\n",
      "  Running command git rev-parse -q --verify 'sha^e536616888d51b453ed354a6f1e243fecb02ea08'\n",
      "  Running command git fetch -q https://github.com/huggingface/peft.git e536616888d51b453ed354a6f1e243fecb02ea08\n",
      "  Running command git checkout -q e536616888d51b453ed354a6f1e243fecb02ea08\n",
      "  Resolved https://github.com/huggingface/peft.git to commit e536616888d51b453ed354a6f1e243fecb02ea08\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from peft==0.3.0.dev0) (22.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/anaconda3/lib/python3.10/site-packages (from peft==0.3.0.dev0) (6.0)\n",
      "Requirement already satisfied: transformers in /student/opt534/.local/lib/python3.10/site-packages (from peft==0.3.0.dev0) (4.41.0.dev0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from peft==0.3.0.dev0) (2.0.1)\n",
      "Requirement already satisfied: accelerate in /student/opt534/.local/lib/python3.10/site-packages (from peft==0.3.0.dev0) (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from peft==0.3.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: psutil in /usr/local/anaconda3/lib/python3.10/site-packages (from peft==0.3.0.dev0) (5.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (2.14.3)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: networkx in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (2.8.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.4.0.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.7.4.91)\n",
      "Requirement already satisfied: sympy in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /usr/local/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0.dev0) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0.dev0) (65.6.3)\n",
      "Requirement already satisfied: lit in /usr/local/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0.dev0) (16.0.3)\n",
      "Requirement already satisfied: cmake in /usr/local/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0.dev0) (3.26.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /student/opt534/.local/lib/python3.10/site-packages (from accelerate->peft==0.3.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub in /student/opt534/.local/lib/python3.10/site-packages (from accelerate->peft==0.3.0.dev0) (0.21.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /student/opt534/.local/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda3/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /student/opt534/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate->peft==0.3.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.3.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch>=1.13.0->peft==0.3.0.dev0) (1.2.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets==2.18.0 in /student/opt534/.local/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from datasets==2.18.0) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from datasets==2.18.0) (6.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (0.6)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.10/site-packages (from datasets==2.18.0) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from datasets==2.18.0) (1.23.5)\n",
      "Requirement already satisfied: multiprocess in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (0.21.4)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.10/site-packages (from datasets==2.18.0) (22.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (0.3.8)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from datasets==2.18.0) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (2024.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/anaconda3/lib/python3.10/site-packages (from datasets==2.18.0) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (15.0.1)\n",
      "Requirement already satisfied: xxhash in /student/opt534/.local/lib/python3.10/site-packages (from datasets==2.18.0) (3.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /student/opt534/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /student/opt534/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /student/opt534/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /student/opt534/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /student/opt534/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.18.0) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /student/opt534/.local/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /student/opt534/.local/lib/python3.10/site-packages (from wandb) (1.41.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /student/opt534/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (4.23.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: PyYAML in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: setproctitle in /student/opt534/.local/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.10/site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git@main bitsandbytes accelerate==0.27.2 #0.20.3  # we need latest transformers for this\n",
    "!pip install git+https://github.com/huggingface/peft.git@e536616888d51b453ed354a6f1e243fecb02ea08\n",
    "!pip install datasets==2.18.0 #2.10.1\n",
    "import locale # colab workaround\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\" # colab workaround\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331a19cd-6a18-4c9d-9a9f-2e4f59ca5937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /student/opt534/.local/lib/python3.10/site-packages (0.21.4)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /student/opt534/.local/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/anaconda3/lib/python3.10/site-packages (from huggingface_hub) (22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22730aa5-1d17-44c6-9849-9cfa8281d07f",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eccf0136-91a2-4fbd-b1e0-52b59329ad65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:37:08.771976: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-23 17:37:08.815614: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb15d0-43a7-4ba2-a512-1c5a7bdc3459",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bc0e9f9-435b-4f14-80c3-70fe61b18aef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"YabTad/CodeRev_BiData\", split=\"train\")\n",
    "\n",
    "data_train = dataset.train_test_split(train_size=0.9)[\"train\"]\n",
    "data_eval = dataset.train_test_split(train_size=0.1)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71559315-020d-45c0-8a67-f3d53b350e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test = load_dataset(\"YabTad/CodeRev_BiData\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c131feac-b91d-435d-9548-e129a93d5d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['OriginalCode', 'ReviewedCode'],\n",
       "    num_rows: 13927\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6582cc-4a87-44b0-a919-00d72fb8c507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['OriginalCode', 'ReviewedCode'],\n",
       "    num_rows: 1547\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e913e33-2c9a-4171-afe4-992ab539d01b",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f245d4d-9810-4493-ae53-22d5fcc0c4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8c63c85c254899b492bd8e37d0807d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec28ba530aab416ca0b6cda53b77f9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb1eab-3d84-4739-bec4-5b3347cf8935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa4ac9e-17d9-4844-9f41-3aa53ee1cba9",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Setup some tokenization settings like left padding because it makes training use less memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8da0881-2150-498d-b4e2-e297abbcbdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170589a-a702-4b4a-8c37-6d5b31f036bc",
   "metadata": {},
   "source": [
    "Setup the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning](https://neptune.ai/blog/self-supervised-learning) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "142ce431-667e-413a-a73d-c5c359621de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfd3ac-0544-41bb-96b9-5b75bec39999",
   "metadata": {},
   "source": [
    "And run convert each data_point into a prompt that I found online that works quite well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d602bc47-2acf-4804-8bea-122ce0295402",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['OriginalCode', 'ReviewedCode'],\n",
       "    num_rows: 13927\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76d7dd5-d64d-43c3-a9ec-ad34a68d735b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"You are a code reviewing model. Your job is to improve Java code. You are given a Java method code as input. \n",
    "\n",
    "You must output the improved version of the code provided as input.\n",
    "\n",
    "### Input:\n",
    "{data_point[\"OriginalCode\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"ReviewedCode\"]}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0ee0a-ae02-4e5b-a4b3-b463eb538898",
   "metadata": {},
   "source": [
    "Reformat to prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e63e5a4-8e53-4f2d-8a12-c2d8b61e4011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12edb91982dd4224afa59d2c49ae473e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13927 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7422486956ef4f4bac1ee9de2ef13f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1547 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = data_train.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = data_eval.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d20d16f2-410b-4601-b29a-9e1f111c7c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['OriginalCode', 'ReviewedCode', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 13927\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10db08ed-b7be-4948-a67c-a42d510931fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['OriginalCode', 'ReviewedCode', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1547\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf670ba-b19b-4e5b-846f-4c8c00f120ef",
   "metadata": {},
   "source": [
    "### Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41a7e717-0518-46a9-ae71-0216fb539598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train() # put model back into training mode\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61a661-862f-40f2-aa7c-74dca6ab1bf5",
   "metadata": {},
   "source": [
    "To resume from a checkpoint, set resume_from_checkpoint to the path of the adapter_model.bin you want to resume from. This code'll replace the lora adapter attached to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9fe30cb-f2fd-48ea-85e6-7ccc55302c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint = \"\" # set this to the adapter_model.bin file you want to resume from\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    if os.path.exists(resume_from_checkpoint):\n",
    "        print(f\"Restarting from {resume_from_checkpoint}\")\n",
    "        adapters_weights = torch.load(resume_from_checkpoint)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {resume_from_checkpoint} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12be4b-382e-4038-bed1-5287dfddef82",
   "metadata": {},
   "source": [
    "Optional stuff to setup Weights and Biases to view training graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e32f0bb-44f7-471e-abe6-f31927c9bb25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_project = \"Bi_CodeReview_codeLlama\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    os.environ['WANDB_NOTEBOOK_NAME'] = 'CodeReviewBi_CodeLlama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac849844-0472-48be-afc7-ccb2a79cc9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "495f38ef-08ee-4f52-aa4b-36959f420288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67974225-9e84-496a-91d4-f4d61f545cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c01ac9da-017b-45bb-a6ba-cbc51152d3de",
   "metadata": {},
   "source": [
    "### Training arguments\n",
    "If you run out of GPU memory, change per_device_train_batch_size. The gradient_accumulation_steps variable should ensure this doesn't affect batch dynamics during the training run. All the other variables are standard stuff that I wouldn't recommend messing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc7850c5-ed41-49fe-86c4-422bfd524f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/opt534/.local/lib/python3.10/site-packages/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "per_device_train_batch_size = 32\n",
    "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
    "output_dir = \"BiCodeReview-llama-FinalRun_f\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        max_steps=400,\n",
    "        learning_rate=3e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_steps=20,\n",
    "        output_dir=output_dir,\n",
    "        load_best_model_at_end=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1995b-9535-4040-ac72-c2a2048068bd",
   "metadata": {},
   "source": [
    "Then we do some pytorch-related optimisation (which just make training faster but don't affect accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb1571a-eb9e-4ade-a382-9a7a5b716ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the model\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "    model, type(model)\n",
    ")\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    print(\"compiling the model\")\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ac56a58-8500-492c-ae13-0732888f0a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find CodeReviewBi_CodeLlama.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtadesse-yeabsira18\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/student/opt534/Documents/CMPT 816/Project/wandb/run-20240423_173934-1qly72vb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tadesse-yeabsira18/Bi_CodeReview_codeLlama/runs/1qly72vb' target=\"_blank\">codellama-2024-04-23-17-39</a></strong> to <a href='https://wandb.ai/tadesse-yeabsira18/Bi_CodeReview_codeLlama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tadesse-yeabsira18/Bi_CodeReview_codeLlama' target=\"_blank\">https://wandb.ai/tadesse-yeabsira18/Bi_CodeReview_codeLlama</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tadesse-yeabsira18/Bi_CodeReview_codeLlama/runs/1qly72vb' target=\"_blank\">https://wandb.ai/tadesse-yeabsira18/Bi_CodeReview_codeLlama/runs/1qly72vb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 2:11:47, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.243200</td>\n",
       "      <td>1.079924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.537200</td>\n",
       "      <td>0.442265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.318545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.289387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.260709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.241371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.238494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.230341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.225947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.225471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.227497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.219676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.219178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.218791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.218345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.215604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.215290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.215369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.214135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.213970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.31450404465198517, metrics={'train_runtime': 7947.7446, 'train_samples_per_second': 6.442, 'train_steps_per_second': 0.05, 'total_flos': 5.201427721833677e+17, 'train_loss': 0.31450404465198517, 'epoch': 3.669724770642202})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0db8f1a6-0211-478a-b066-57d15a1f58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bi_finetuned-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
